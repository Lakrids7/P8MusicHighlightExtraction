#!/usr/bin/env python3
# train_salami_chorus_integrated.py
import os, json, math, argparse, re
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split # For creating split

# --- Constants ---
N_MELS, CHUNK_FRAMES, TARGET_DIM, MEL_SUFFIX = 128, 128, 2, ".npy"

# --- Configuration for Data Paths (adjust as needed) ---
# Path to the CSV generated by preprocess_salami_data.py
AUGMENTED_CHORUS_CSV_FOR_TRAINING = "salami_chorus_annotations_with_duration.csv"
# Base directory where precomputed Mel spectrograms are stored
PRECOMPUTED_MEL_BASE_DIR = "data/salami_mels" # Example: data/salami_mels/3/full_song.npy
# Default path for the train/val/test split JSON
DEFAULT_SPLIT_JSON_PATH = "salami_chorus_split.json"

# --- Global DataFrame ---
CHORUS_ANNOTATIONS_DF = None

def load_chorus_annotations_globally(csv_path):
    global CHORUS_ANNOTATIONS_DF
    if CHORUS_ANNOTATIONS_DF is None:
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"Augmented chorus CSV not found at {csv_path}")
        CHORUS_ANNOTATIONS_DF = pd.read_csv(csv_path)
        CHORUS_ANNOTATIONS_DF['salami_id'] = CHORUS_ANNOTATIONS_DF['salami_id'].astype(str)
        CHORUS_ANNOTATIONS_DF = CHORUS_ANNOTATIONS_DF[CHORUS_ANNOTATIONS_DF['total_song_duration_sec'] > 0.1]
        CHORUS_ANNOTATIONS_DF = CHORUS_ANNOTATIONS_DF[CHORUS_ANNOTATIONS_DF['chorus_duration'] > 0.05]
        if CHORUS_ANNOTATIONS_DF.empty:
            raise ValueError(f"No valid data in CSV {csv_path} after filtering.")
        print(f"Loaded {len(CHORUS_ANNOTATIONS_DF)} chorus entries globally from {csv_path}.")


# --- Function to Create or Load Split ---
def get_or_create_data_split(split_json_path, mel_base_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    if os.path.exists(split_json_path):
        print(f"Loading existing data split from: {split_json_path}")
        with open(split_json_path, 'r') as f:
            split_data = json.load(f)
        # Quick validation
        if not all(k in split_data for k in ["train", "val", "test"]):
            print(f"Warning: Existing split file {split_json_path} is malformed. Recreating.")
        else:
            # Check if paths in split file actually exist (optional, for robustness)
            # This assumes items in split_data are directories like PRECOMPUTED_MEL_BASE_DIR/salami_id
            # Example check for one training item:
            if split_data["train"]:
                first_train_item_mel = os.path.join(split_data["train"][0], f"full_song{MEL_SUFFIX}")
                if not os.path.exists(first_train_item_mel):
                    print(f"Warning: Mel file for first item in existing split ({first_train_item_mel}) not found. Consider recreating split if mels moved.")
            return split_data

    print(f"Creating new data split: {split_json_path}")
    if CHORUS_ANNOTATIONS_DF is None:
        raise RuntimeError("Chorus annotations DataFrame not loaded. Call load_chorus_annotations_globally() first.")

    unique_salami_ids = CHORUS_ANNOTATIONS_DF['salami_id'].unique().tolist()
    
    # Filter unique_salami_ids to only include those with existing mel files
    valid_salami_ids = []
    for sid in unique_salami_ids:
        mel_path_check = os.path.join(mel_base_dir, str(sid), f"full_song{MEL_SUFFIX}")
        if os.path.exists(mel_path_check):
            valid_salami_ids.append(sid)
        # else:
            # print(f"Debug: Mel file not found for salami_id {sid} at {mel_path_check}, excluding from split.")
    
    if not valid_salami_ids:
        raise ValueError("No salami_ids with corresponding mel files found. Check PRECOMPUTED_MEL_BASE_DIR.")
    
    print(f"Found {len(valid_salami_ids)} unique songs with chorus annotations AND existing mel files.")


    if len(valid_salami_ids) < 3:
        print("Warning: Not enough unique songs with mel files to create a meaningful split (need at least 3).")
        # Create a minimal split if possible
        if len(valid_salami_ids) == 1:
             train_ids, val_ids, test_ids = valid_salami_ids, [], []
        elif len(valid_salami_ids) == 2:
             train_ids, val_ids, test_ids = [valid_salami_ids[0]], [valid_salami_ids[1]], []
        else: # 0 valid ids
             train_ids, val_ids, test_ids = [], [], []

    else:
        np.random.shuffle(valid_salami_ids)
        train_ids, temp_ids = train_test_split(valid_salami_ids, test_size=(val_ratio + test_ratio), random_state=42)
        if len(temp_ids) > 1:
            relative_test_ratio = test_ratio / (val_ratio + test_ratio)
            val_ids, test_ids = train_test_split(temp_ids, test_size=relative_test_ratio, random_state=42)
        elif len(temp_ids) == 1:
            val_ids, test_ids = temp_ids, []
        else:
            val_ids, test_ids = [], []


    def ids_to_datapaths(ids_list):
        return [os.path.join(mel_base_dir, str(sid)) for sid in ids_list]

    split_data = {
        "train": ids_to_datapaths(train_ids),
        "val": ids_to_datapaths(val_ids),
        "test": ids_to_datapaths(test_ids)
    }

    try:
        os.makedirs(os.path.dirname(split_json_path) or '.', exist_ok=True) # Ensure dir exists
        with open(split_json_path, 'w') as f:
            json.dump(split_data, f, indent=4)
        print(f"Successfully created and saved split file: {split_json_path}")
        print(f"Train: {len(train_ids)} songs, Val: {len(val_ids)} songs, Test: {len(test_ids)} songs")
    except IOError as e:
        raise IOError(f"Could not write to JSON file: {split_json_path}. Error: {e}")
    
    return split_data


# --- Helper: parse_salami_chorus_label ---
def parse_salami_chorus_label(song_dir_path):
    global CHORUS_ANNOTATIONS_DF
    if CHORUS_ANNOTATIONS_DF is None: return None
    try:
        salami_id = os.path.basename(song_dir_path)
    except Exception: return None

    song_choruses = CHORUS_ANNOTATIONS_DF[CHORUS_ANNOTATIONS_DF['salami_id'] == salami_id]
    if song_choruses.empty: return None

    first_chorus = song_choruses.iloc[0]
    start_time = float(first_chorus['chorus_start_time'])
    end_time = float(first_chorus['chorus_end_time'])
    total_dur = float(first_chorus['total_song_duration_sec'])

    if total_dur > 0 and end_time > start_time:
        norm_start = max(0.0, min(1.0, start_time / total_dur))
        norm_end = max(0.0, min(1.0, end_time / total_dur))
        if norm_end > norm_start:
            return np.array([norm_start, norm_end], dtype=np.float32)
    return None

# --- Dataset ---
class SongDataset(Dataset):
    def __init__(self, song_dirs): # song_dirs are paths like '.../PRECOMPUTED_MEL_BASE_DIR/salami_id'
        self.items = []
        # CHORUS_ANNOTATIONS_DF should be loaded globally before SongDataset instantiation
        if CHORUS_ANNOTATIONS_DF is None:
            raise RuntimeError("SongDataset initialized before CHORUS_ANNOTATIONS_DF was loaded.")

        for d in song_dirs:
            mel_path = os.path.join(d, f"full_song{MEL_SUFFIX}")
            if os.path.exists(mel_path):
                # Pre-check if label exists for this song_dir to avoid adding items that will fail in __getitem__
                if parse_salami_chorus_label(d) is not None:
                     self.items.append((mel_path, d))
                # else:
                    # print(f"Debug: No valid label for {d}, skipping.") # Potentially noisy
            # else:
                # print(f"Debug: Mel file not found {mel_path}, skipping.") # Potentially noisy

    def __len__(self): return len(self.items)

    def __getitem__(self, idx):
        mel_path, song_dir_path = self.items[idx]
        try:
            mel_chunks = np.load(mel_path) # Expected: (num_chunks, N_MELS, CHUNK_FRAMES)
            label = parse_salami_chorus_label(song_dir_path)

            if label is not None and mel_chunks.ndim == 3 and \
               mel_chunks.shape[1:] == (N_MELS, CHUNK_FRAMES) and mel_chunks.shape[0] > 0:
                return torch.from_numpy(mel_chunks.astype(np.float32)), torch.from_numpy(label)
        except Exception as e:
            print(f"Error in __getitem__ for {mel_path} or {song_dir_path}: {e}")
        return None

# --- Collate ---
def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if not batch: return None, None, None
    mels, labels = zip(*batch)
    lengths = torch.tensor([mel.shape[0] for mel in mels])
    padded_mels = pad_sequence(mels, batch_first=True, padding_value=0.0)
    labels = torch.stack(labels)
    return padded_mels, labels, lengths

# --- Model (MusicHighlighter and positional_encoding are unchanged) ---
def positional_encoding(L, D, device):
    pe = torch.zeros(L, D, device=device)
    pos = torch.arange(0, L, dtype=torch.float, device=device).unsqueeze(1)
    div = torch.exp(torch.arange(0, D, 2, device=device) * (-math.log(10000.0) / D))
    pe[:, 0::2] = torch.sin(pos * div); pe[:, 1::2] = torch.cos(pos * div)
    return pe

class MusicHighlighter(nn.Module):
    def __init__(self, dim=64):
        super().__init__(); self.feat_dim = dim * 4
        def conv(ic, oc, k, s): return nn.Sequential(nn.Conv2d(ic, oc, k, s), nn.BatchNorm2d(oc), nn.ReLU())
        self.conv_blocks = nn.Sequential(conv(1, dim, (3, N_MELS), (2, 1)), conv(dim, dim*2, (4, 1), (2, 1)), conv(dim*2, self.feat_dim, (4, 1), (2, 1)))
        self.attn_mlp = nn.Sequential(nn.Linear(self.feat_dim, self.feat_dim), nn.ReLU(), nn.Dropout(0.5), nn.Linear(self.feat_dim, self.feat_dim), nn.Tanh(), nn.Dropout(0.5), nn.Linear(self.feat_dim, 1))
        self.regr_head = nn.Sequential(nn.Linear(self.feat_dim, 128), nn.ReLU(), nn.Dropout(0.5), nn.Linear(128, TARGET_DIM), nn.Sigmoid())

    def forward(self, x, lengths): # x: (B, L_max_chunks, N_MELS, CHUNK_FRAMES)
        B, L_max_chunks, M_n_mels, T_chunk_frames = x.shape; dev = x.device
        x_reshaped = x.permute(0, 1, 3, 2).reshape(B * L_max_chunks, 1, T_chunk_frames, M_n_mels)
        h_chunks = self.conv_blocks(x_reshaped)
        h_chunks = torch.max(h_chunks.squeeze(3), dim=2)[0]
        h_t = h_chunks.view(B, L_max_chunks, -1)
        pe = positional_encoding(L_max_chunks, self.feat_dim, dev).unsqueeze(0)
        attn_logits = self.attn_mlp((h_t + pe).view(B * L_max_chunks, -1)).view(B, L_max_chunks)
        mask = torch.arange(L_max_chunks, device=dev)[None, :] >= lengths[:, None]
        attn_logits.masked_fill_(mask, -float('inf'))
        alpha_t = torch.softmax(attn_logits, dim=1).unsqueeze(1)
        weighted_features = torch.bmm(alpha_t, h_t).squeeze(1)
        return self.regr_head(weighted_features), alpha_t.squeeze(1)

# --- Training Loop ---
def run_training(args):
    device = "cpu" if args.cpu or not torch.cuda.is_available() else "cuda"

    try:
        load_chorus_annotations_globally(args.csv)
        split_data = get_or_create_data_split(args.split, args.mel_dir,
                                              args.train_ratio, args.val_ratio, args.test_ratio)
    except Exception as e:
        print(f"Error during data setup: {e}")
        return

    train_song_dirs = split_data.get("train", [])
    val_song_dirs = split_data.get("val", [])

    if not train_song_dirs:
        print("No training data paths found in the split. Aborting.")
        return

    train_ds = SongDataset(train_song_dirs)
    val_ds = SongDataset(val_song_dirs) # Will be empty if val_song_dirs is empty

    if not train_ds or len(train_ds) == 0:
        print("No valid training samples found after SongDataset processing. Check paths and data.")
        return
    print(f"Using {len(train_ds)} train, {len(val_ds)} val samples.")

    ds_args = {'batch_size': args.batch, 'num_workers': args.workers, 'pin_memory': True,
               'collate_fn': collate_fn, 'persistent_workers': args.workers > 0}
    train_ld = DataLoader(train_ds, shuffle=True, **ds_args)
    val_ld = DataLoader(val_ds, shuffle=False, **ds_args) if len(val_ds) > 0 else None

    model = MusicHighlighter().to(device)
    opt = torch.optim.Adam(model.parameters(), lr=args.lr)
    crit = nn.MSELoss()
    best_val_loss = float("inf")

    print(f"Starting training on {device}...")
    for ep in range(1, args.epochs + 1):
        model.train()
        train_loss, train_n = 0.0, 0
        for mel_padded, y_labels, chunk_lengths in train_ld:
            if mel_padded is None: continue
            mel_padded, y_labels, chunk_lengths = mel_padded.to(device), y_labels.to(device), chunk_lengths.to(device)
            opt.zero_grad()
            pred, _ = model(mel_padded, chunk_lengths)
            loss = crit(pred, y_labels)
            loss.backward(); opt.step()
            train_loss += loss.item() * mel_padded.size(0); train_n += mel_padded.size(0)
        avg_train_loss = train_loss / train_n if train_n > 0 else 0

        avg_val_loss = float('inf')
        if val_ld: # Only validate if there's validation data
            model.eval()
            val_loss, val_n = 0.0, 0
            with torch.no_grad():
                for mel_padded, y_labels, chunk_lengths in val_ld:
                    if mel_padded is None: continue
                    mel_padded, y_labels, chunk_lengths = mel_padded.to(device), y_labels.to(device), chunk_lengths.to(device)
                    pred, _ = model(mel_padded, chunk_lengths)
                    loss = crit(pred, y_labels)
                    val_loss += loss.item() * mel_padded.size(0); val_n += mel_padded.size(0)
            avg_val_loss = val_loss / val_n if val_n > 0 else float('inf')
        
        print(f"E {ep:02d}/{args.epochs} | Tr Loss: {avg_train_loss:.4f} | Vl Loss: {avg_val_loss:.4f if avg_val_loss != float('inf') else 'N/A'}", end="")
        if avg_val_loss < best_val_loss and val_ld: # Save only if val_loss improved and val_ld exists
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), args.out)
            print(f" -> Saved {args.out}")
        else:
            print()
            
    print(f"Done. Best Val Loss: {best_val_loss:.4f if best_val_loss != float('inf') else 'N/A'}")


# --- CLI ---
if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Train MusicHighlighter on SALAMI Chorus Data (Integrated Split)")
    p.add_argument("--csv", default=AUGMENTED_CHORUS_CSV_FOR_TRAINING, help="Path to augmented SALAMI chorus CSV")
    p.add_argument("--mel_dir", default=PRECOMPUTED_MEL_BASE_DIR, help="Base directory of precomputed Mel spectrograms")
    p.add_argument("--split", default=DEFAULT_SPLIT_JSON_PATH, help="Path to load/save data split JSON")
    p.add_argument("--epochs", type=int, default=50, help="Epochs")
    p.add_argument("--batch", type=int, default=8, help="Batch size")
    p.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    p.add_argument("--cpu", action="store_true", help="Force CPU")
    p.add_argument("--workers", type=int, default=0, help="DataLoader workers")
    p.add_argument("--out", default="salami_chorus_highlighter_integrated.pt", help="Output model path")
    p.add_argument("--train_ratio", type=float, default=0.7, help="Train split ratio")
    p.add_argument("--val_ratio", type=float, default=0.15, help="Validation split ratio")
    p.add_argument("--test_ratio", type=float, default=0.15, help="Test split ratio (sum with train/val should be 1.0)")
    args = p.parse_args()

    if not (0 < args.train_ratio < 1 and 0 <= args.val_ratio < 1 and 0 <= args.test_ratio < 1 and \
            math.isclose(args.train_ratio + args.val_ratio + args.test_ratio, 1.0)):
        p.error("Train, validation, and test ratios must be between 0 and 1, and sum to 1.0.")

    # Create mel_dir if it doesn't exist (user needs to populate it)
    os.makedirs(args.mel_dir, exist_ok=True)
    print(f"Ensure precomputed Mel spectrograms are in subdirectories under: {args.mel_dir}")
    print(f"e.g., {args.mel_dir}/<salami_id>/full_song.npy")
    print(f"Make sure {args.csv} exists or run preprocess_salami_data.py first.")

    run_training(args)